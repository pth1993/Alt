nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/hoang/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sat Oct  8 17:10:26 ICT 2016
Executing: mkdir -p /home/hoang/Desktop/Alt+/moses/working/train/corpus
(1.0) selecting factors @ Sat Oct  8 17:10:26 ICT 2016
(1.1) running mkcls  @ Sat Oct  8 17:10:26 ICT 2016
/home/hoang/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hoang/Desktop/Alt+/moses/data/train/clean.train.word -V/home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb.classes opt
Executing: /home/hoang/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hoang/Desktop/Alt+/moses/data/train/clean.train.word -V/home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 18839

start-costs: MEAN: 2.43605e+06 (2.43461e+06-2.4375e+06)  SIGMA:1443.08   
  end-costs: MEAN: 2.22433e+06 (2.22256e+06-2.2261e+06)  SIGMA:1768.98   
   start-pp: MEAN: 632.892 (628.395-637.39)  SIGMA:4.49743   
     end-pp: MEAN: 223.12 (221.176-225.064)  SIGMA:1.94358   
 iterations: MEAN: 482704 (454724-510685)  SIGMA:27980.5   
       time: MEAN: 7.312 (6.776-7.848)  SIGMA:0.536   
(1.1) running mkcls  @ Sat Oct  8 17:10:41 ICT 2016
/home/hoang/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag -V/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb.classes opt
Executing: /home/hoang/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag -V/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb.classes opt
WARNING: StatVar.cc
WARNING: StatVar.cc

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 47

start-costs: MEAN: 2.36767e+06 (2.366e+06-2.36934e+06)  SIGMA:1668.11   
  end-costs: MEAN: 2.33329e+06 (2.33329e+06-2.33329e+06)  SIGMA:0.0441942   
   start-pp: MEAN: 11.199 (11.107-11.291)  SIGMA:0.0919912   
     end-pp: MEAN: 9.4544 (9.4544-9.4544)  SIGMA:0   
 iterations: MEAN: 50087.5 (50084-50091)  SIGMA:3.5   
       time: MEAN: 3.742 (3.74-3.744)  SIGMA:0.002   
(1.2) creating vcb file /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb @ Sat Oct  8 17:10:49 ICT 2016
(1.2) creating vcb file /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb @ Sat Oct  8 17:10:49 ICT 2016
(1.3) numberizing corpus /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt @ Sat Oct  8 17:10:49 ICT 2016
(1.3) numberizing corpus /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt @ Sat Oct  8 17:10:49 ICT 2016
(2) running giza @ Sat Oct  8 17:10:49 ICT 2016
(2.1a) running snt2cooc word-tag @ Sat Oct  8 17:10:49 ICT 2016

Executing: mkdir -p /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag
Executing: /home/hoang/mosesdecoder/tools/snt2cooc.out /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt > /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.cooc
/home/hoang/mosesdecoder/tools/snt2cooc.out /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt > /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
END.
(2.1b) running giza word-tag @ Sat Oct  8 17:10:50 ICT 2016
/home/hoang/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.cooc -c /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag -onlyaldumps 1 -p0 0.999 -s /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb -t /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb
Executing: /home/hoang/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.cooc -c /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag -onlyaldumps 1 -p0 0.999 -s /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb -t /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb
/home/hoang/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.cooc -c /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag -onlyaldumps 1 -p0 0.999 -s /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb -t /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb
Parameter 'coocurrencefile' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.cooc'
Parameter 'c' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '116-10-08.171050.hoang' to '/home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb'
Parameter 't' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 116-10-08.171050.hoang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb  (source vocabulary file name)
t = /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 116-10-08.171050.hoang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb  (source vocabulary file name)
t = /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb
Reading vocabulary file from:/home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb
Source vocabulary list has 48 unique tokens 
Target vocabulary list has 18840 unique tokens 
Calculating vocabulary frequencies from corpus /home/hoang/Desktop/Alt+/moses/working/train/corpus/word-tag-int-train.snt
Reading more sentence pairs into memory ... 
{WARNING:(a)truncated sentence 13931}{WARNING:(b)truncated sentence 13931}Corpus fits in memory, corpus has: 14000 sentence pairs.
 Train total # sentence pairs (weighted): 14000
Size of source portion of the training corpus: 189058 tokens
Size of the target portion of the training corpus: 189058 tokens 
In source portion of the training corpus, only 47 unique tokens appeared
In target portion of the training corpus, only 18838 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 189058/(203058-14000)== 1
There are 318552 318552 entries in table
==========================================================
Model1 Training Started at: Sat Oct  8 17:10:50 2016

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 14.4225 PERPLEXITY 21958.6
Model1: (1) VITERBI TRAIN CROSS-ENTROPY inf PERPLEXITY inf
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 9.46346 PERPLEXITY 705.967
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 12.6103 PERPLEXITY 6252.72
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 9.17865 PERPLEXITY 579.493
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 11.8091 PERPLEXITY 3588.35
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 9.02226 PERPLEXITY 519.96
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 11.3413 PERPLEXITY 2594.55
Model 1 Iteration: 4 took: 1 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 8.93178 PERPLEXITY 488.352
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 11.0453 PERPLEXITY 2113.32
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 2 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 47  #classes: 47
Read classes: #words: 18839  #classes: 51

==========================================================
Hmm Training Started at: Sat Oct  8 17:10:52 2016

-----------
Hmm: Iteration 1
A/D table contains 95808 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 8.87242 PERPLEXITY 468.666
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY inf PERPLEXITY inf

Hmm Iteration: 1 took: 5 seconds

-----------
Hmm: Iteration 2
A/D table contains 95808 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 8.7064 PERPLEXITY 417.721
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 9.78753 PERPLEXITY 883.772

Hmm Iteration: 2 took: 4 seconds

-----------
Hmm: Iteration 3
A/D table contains 95808 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 8.17435 PERPLEXITY 288.884
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 8.74433 PERPLEXITY 428.851

Hmm Iteration: 3 took: 4 seconds

-----------
Hmm: Iteration 4
A/D table contains 95808 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 7.58654 PERPLEXITY 192.21
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 7.8403 PERPLEXITY 229.173

Hmm Iteration: 4 took: 5 seconds

-----------
Hmm: Iteration 5
A/D table contains 95808 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 7.0401 PERPLEXITY 131.608
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 7.16791 PERPLEXITY 143.799

Hmm Iteration: 5 took: 4 seconds

Entire Hmm Training took: 22 seconds
==========================================================
Read classes: #words: 47  #classes: 47
Read classes: #words: 18839  #classes: 51
Read classes: #words: 47  #classes: 47
Read classes: #words: 18839  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Oct  8 17:11:14 2016


---------------------
THTo3: Iteration 1
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 467.454 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 95808 parameters.
A/D table contains 95489 parameters.
NTable contains 480 parameter.
p0_count is 181848 and p1 is 3590.79; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 6.61359 PERPLEXITY 97.9242
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 6.6584 PERPLEXITY 101.013

THTo3 Viterbi Iteration : 1 took: 3 seconds

---------------------
Model3: Iteration 2
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 468.855 #alsophisticatedcountcollection: 0 #hcsteps: 1.70021
#peggingImprovements: 0
A/D table contains 95808 parameters.
A/D table contains 95607 parameters.
NTable contains 480 parameter.
p0_count is 186977 and p1 is 1040.53; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 6.48953 PERPLEXITY 89.8554
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 6.51061 PERPLEXITY 91.1779

Model3 Viterbi Iteration : 2 took: 2 seconds

---------------------
Model3: Iteration 3
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.292 #alsophisticatedcountcollection: 0 #hcsteps: 1.52864
#peggingImprovements: 0
A/D table contains 95808 parameters.
A/D table contains 95707 parameters.
NTable contains 480 parameter.
p0_count is 188545 and p1 is 256.502; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 6.22436 PERPLEXITY 74.7686
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 6.23388 PERPLEXITY 75.2637

Model3 Viterbi Iteration : 3 took: 3 seconds

---------------------
T3To4: Iteration 4
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.571 #alsophisticatedcountcollection: 4.09343 #hcsteps: 1.35579
#peggingImprovements: 0
D4 table contains 358701 parameters.
A/D table contains 95808 parameters.
A/D table contains 95707 parameters.
NTable contains 480 parameter.
p0_count is 188771 and p1 is 143.645; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 6.07899 PERPLEXITY 67.6018
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 6.0837 PERPLEXITY 67.823

T3To4 Viterbi Iteration : 4 took: 3 seconds

---------------------
Model4: Iteration 5
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.661 #alsophisticatedcountcollection: 3.708 #hcsteps: 1.14964
#peggingImprovements: 0
D4 table contains 358701 parameters.
A/D table contains 95808 parameters.
A/D table contains 95632 parameters.
NTable contains 480 parameter.
p0_count is 188698 and p1 is 179.996; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.70868 PERPLEXITY 104.596
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 6.71444 PERPLEXITY 105.014

Model4 Viterbi Iteration : 5 took: 6 seconds

---------------------
Model4: Iteration 6
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.688 #alsophisticatedcountcollection: 3.35293 #hcsteps: 1.10086
#peggingImprovements: 0
D4 table contains 358701 parameters.
A/D table contains 95808 parameters.
A/D table contains 95405 parameters.
NTable contains 480 parameter.
p0_count is 188692 and p1 is 183.205; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.68747 PERPLEXITY 103.069
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 6.69247 PERPLEXITY 103.427

Model4 Viterbi Iteration : 6 took: 6 seconds
H333444 Training Finished at: Sat Oct  8 17:11:37 2016


Entire Viterbi H333444 Training took: 23 seconds
==========================================================

Entire Training took: 47 seconds
Program Finished at: Sat Oct  8 17:11:37 2016

==========================================================
Executing: rm -f /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.A3.final.gz
Executing: gzip /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.A3.final
(2.1a) running snt2cooc tag-word @ Sat Oct  8 17:11:37 ICT 2016

Executing: mkdir -p /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word
Executing: /home/hoang/mosesdecoder/tools/snt2cooc.out /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt > /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.cooc
/home/hoang/mosesdecoder/tools/snt2cooc.out /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt > /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
END.
(2.1b) running giza tag-word @ Sat Oct  8 17:11:37 ICT 2016
/home/hoang/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.cooc -c /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word -onlyaldumps 1 -p0 0.999 -s /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb -t /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb
Executing: /home/hoang/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.cooc -c /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word -onlyaldumps 1 -p0 0.999 -s /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb -t /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb
/home/hoang/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.cooc -c /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word -onlyaldumps 1 -p0 0.999 -s /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb -t /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb
Parameter 'coocurrencefile' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.cooc'
Parameter 'c' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '116-10-08.171137.hoang' to '/home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb'
Parameter 't' changed from '' to '/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 116-10-08.171137.hoang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb  (source vocabulary file name)
t = /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 116-10-08.171137.hoang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb  (source vocabulary file name)
t = /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/hoang/Desktop/Alt+/moses/working/train/corpus/word.vcb
Reading vocabulary file from:/home/hoang/Desktop/Alt+/moses/working/train/corpus/tag.vcb
Source vocabulary list has 18840 unique tokens 
Target vocabulary list has 48 unique tokens 
Calculating vocabulary frequencies from corpus /home/hoang/Desktop/Alt+/moses/working/train/corpus/tag-word-int-train.snt
Reading more sentence pairs into memory ... 
{WARNING:(a)truncated sentence 13931}{WARNING:(b)truncated sentence 13931}Corpus fits in memory, corpus has: 14000 sentence pairs.
 Train total # sentence pairs (weighted): 14000
Size of source portion of the training corpus: 189058 tokens
Size of the target portion of the training corpus: 189058 tokens 
In source portion of the training corpus, only 18839 unique tokens appeared
In target portion of the training corpus, only 46 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 189058/(203058-14000)== 1
There are 299760 299760 entries in table
==========================================================
Model1 Training Started at: Sat Oct  8 17:11:37 2016

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 5.80595 PERPLEXITY 55.9454
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 10.1617 PERPLEXITY 1145.48
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 3.80474 PERPLEXITY 13.9747
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.17217 PERPLEXITY 144.224
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 3.6154 PERPLEXITY 12.2559
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.53951 PERPLEXITY 93.0227
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 3.50742 PERPLEXITY 11.372
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 6.10374 PERPLEXITY 68.7714
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 3.4401 PERPLEXITY 10.8536
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.79896 PERPLEXITY 55.675
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 18839  #classes: 51
Read classes: #words: 47  #classes: 47

==========================================================
Hmm Training Started at: Sat Oct  8 17:11:38 2016

-----------
Hmm: Iteration 1
A/D table contains 95808 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 3.38962 PERPLEXITY 10.4804
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY inf PERPLEXITY inf

Hmm Iteration: 1 took: 3 seconds

-----------
Hmm: Iteration 2
A/D table contains 95808 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.15489 PERPLEXITY 8.9067
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.26484 PERPLEXITY 19.224

Hmm Iteration: 2 took: 4 seconds

-----------
Hmm: Iteration 3
A/D table contains 95808 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 2.45232 PERPLEXITY 5.47295
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 2.93991 PERPLEXITY 7.67361

Hmm Iteration: 3 took: 3 seconds

-----------
Hmm: Iteration 4
A/D table contains 95808 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.53734 PERPLEXITY 2.9026
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 1.70912 PERPLEXITY 3.26961

Hmm Iteration: 4 took: 4 seconds

-----------
Hmm: Iteration 5
A/D table contains 95808 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 0.995765 PERPLEXITY 1.99414
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.08207 PERPLEXITY 2.11707

Hmm Iteration: 5 took: 3 seconds

Entire Hmm Training took: 17 seconds
==========================================================
Read classes: #words: 18839  #classes: 51
Read classes: #words: 47  #classes: 47
Read classes: #words: 18839  #classes: 51
Read classes: #words: 47  #classes: 47

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sat Oct  8 17:11:55 2016


---------------------
THTo3: Iteration 1
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.81 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 95808 parameters.
A/D table contains 95807 parameters.
NTable contains 188400 parameter.
p0_count is 183470 and p1 is 2791.63; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 0.692567 PERPLEXITY 1.61616
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 0.722165 PERPLEXITY 1.64966

THTo3 Viterbi Iteration : 1 took: 3 seconds

---------------------
Model3: Iteration 2
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.826 #alsophisticatedcountcollection: 0 #hcsteps: 1.12543
#peggingImprovements: 0
A/D table contains 95808 parameters.
A/D table contains 95807 parameters.
NTable contains 188400 parameter.
p0_count is 188265 and p1 is 396.721; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 0.267974 PERPLEXITY 1.20412
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 0.270051 PERPLEXITY 1.20585

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.829 #alsophisticatedcountcollection: 0 #hcsteps: 1.07836
#peggingImprovements: 0
A/D table contains 95808 parameters.
A/D table contains 95807 parameters.
NTable contains 188400 parameter.
p0_count is 188878 and p1 is 89.7667; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 0.207965 PERPLEXITY 1.15506
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 0.208359 PERPLEXITY 1.15537

Model3 Viterbi Iteration : 3 took: 2 seconds

---------------------
T3To4: Iteration 4
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.829 #alsophisticatedcountcollection: 1.3685 #hcsteps: 1.06786
#peggingImprovements: 0
D4 table contains 346115 parameters.
A/D table contains 95808 parameters.
A/D table contains 95807 parameters.
NTable contains 188400 parameter.
p0_count is 188998 and p1 is 29.8866; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 0.198075 PERPLEXITY 1.14717
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 0.198163 PERPLEXITY 1.14724

T3To4 Viterbi Iteration : 4 took: 2 seconds

---------------------
Model4: Iteration 5
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.829 #alsophisticatedcountcollection: 2.68621 #hcsteps: 1.05686
#peggingImprovements: 0
D4 table contains 346115 parameters.
A/D table contains 95808 parameters.
A/D table contains 94954 parameters.
NTable contains 188400 parameter.
p0_count is 189006 and p1 is 26.1404; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 0.896526 PERPLEXITY 1.86158
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 0.897001 PERPLEXITY 1.86219

Model4 Viterbi Iteration : 5 took: 4 seconds

---------------------
Model4: Iteration 6
10000
#centers(pre/hillclimbed/real): 1 1 1  #al: 469.829 #alsophisticatedcountcollection: 2.69829 #hcsteps: 1.05593
#peggingImprovements: 0
D4 table contains 346115 parameters.
A/D table contains 95808 parameters.
A/D table contains 94962 parameters.
NTable contains 188400 parameter.
p0_count is 188995 and p1 is 31.5438; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 0.8969 PERPLEXITY 1.86206
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 0.897275 PERPLEXITY 1.86254

Model4 Viterbi Iteration : 6 took: 6 seconds
H333444 Training Finished at: Sat Oct  8 17:12:13 2016


Entire Viterbi H333444 Training took: 18 seconds
==========================================================

Entire Training took: 36 seconds
Program Finished at: Sat Oct  8 17:12:13 2016

==========================================================
Executing: rm -f /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.A3.final.gz
Executing: gzip /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.A3.final
(3) generate word alignment @ Sat Oct  8 17:12:13 ICT 2016
Combining forward and inverted alignment from files:
  /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.A3.final.{bz2,gz}
  /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.A3.final.{bz2,gz}
Executing: mkdir -p /home/hoang/Desktop/Alt+/moses/working/train/model
Executing: /home/hoang/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/hoang/Desktop/Alt+/moses/working/train/giza.tag-word/tag-word.A3.final.gz" -i "gzip -cd /home/hoang/Desktop/Alt+/moses/working/train/giza.word-tag/word-tag.A3.final.gz" |/home/hoang/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/hoang/Desktop/Alt+/moses/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<14000>
(4) generate lexical translation table 0-0 @ Sat Oct  8 17:12:14 ICT 2016
(/home/hoang/Desktop/Alt+/moses/data/train/clean.train.word,/home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag,/home/hoang/Desktop/Alt+/moses/working/train/model/lex)
!!!!!!!!!!!!!!
Saved: /home/hoang/Desktop/Alt+/moses/working/train/model/lex.f2e and /home/hoang/Desktop/Alt+/moses/working/train/model/lex.e2f
FILE: /home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag
FILE: /home/hoang/Desktop/Alt+/moses/data/train/clean.train.word
FILE: /home/hoang/Desktop/Alt+/moses/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Sat Oct  8 17:12:15 ICT 2016
/home/hoang/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/hoang/mosesdecoder/scripts/../bin/extract /home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag /home/hoang/Desktop/Alt+/moses/data/train/clean.train.word /home/hoang/Desktop/Alt+/moses/working/train/model/aligned.grow-diag-final-and /home/hoang/Desktop/Alt+/moses/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/hoang/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/hoang/mosesdecoder/scripts/../bin/extract /home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag /home/hoang/Desktop/Alt+/moses/data/train/clean.train.word /home/hoang/Desktop/Alt+/moses/working/train/model/aligned.grow-diag-final-and /home/hoang/Desktop/Alt+/moses/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sat Oct  8 17:12:15 2016
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902; ls -l /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902 
total=14000 line-per-split=3501 
split -d -l 3501 -a 7 /home/hoang/Desktop/Alt+/moses/data/train/clean.train.tag /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/target.split -d -l 3501 -a 7 /home/hoang/Desktop/Alt+/moses/data/train/clean.train.word /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/source.split -d -l 3501 -a 7 /home/hoang/Desktop/Alt+/moses/working/train/model/aligned.grow-diag-final-and /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/align.merging extract / extract.inv
gunzip -c /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000000.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000001.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000002.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000003.gz  | LC_ALL=C sort     -T /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902 2>> /dev/stderr | gzip -c > /home/hoang/Desktop/Alt+/moses/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000000.o.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000001.o.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000002.o.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902 2>> /dev/stderr | gzip -c > /home/hoang/Desktop/Alt+/moses/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
gunzip -c /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000000.inv.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000001.inv.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000002.inv.gz /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15902 2>> /dev/stderr | gzip -c > /home/hoang/Desktop/Alt+/moses/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
Finished Sat Oct  8 17:12:23 2016
(6) score phrases @ Sat Oct  8 17:12:23 ICT 2016
(6.1)  creating table half /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.f2e @ Sat Oct  8 17:12:23 ICT 2016
/home/hoang/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hoang/mosesdecoder/scripts/../bin/score /home/hoang/Desktop/Alt+/moses/working/train/model/extract.sorted.gz /home/hoang/Desktop/Alt+/moses/working/train/model/lex.f2e /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/hoang/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hoang/mosesdecoder/scripts/../bin/score /home/hoang/Desktop/Alt+/moses/working/train/model/extract.sorted.gz /home/hoang/Desktop/Alt+/moses/working/train/model/lex.f2e /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Sat Oct  8 17:12:23 2016
/home/hoang/mosesdecoder/scripts/../bin/score /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/extract.0.gz /home/hoang/Desktop/Alt+/moses/working/train/model/lex.f2e /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/run.0.sh/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/run.1.sh/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/run.3.sh/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/run.2.shmv /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978/phrase-table.half.0000000.gz /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.f2e.gzrm -rf /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.15978 
Finished Sat Oct  8 17:12:33 2016
(6.3)  creating table half /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.e2f @ Sat Oct  8 17:12:33 ICT 2016
/home/hoang/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hoang/mosesdecoder/scripts/../bin/score /home/hoang/Desktop/Alt+/moses/working/train/model/extract.inv.sorted.gz /home/hoang/Desktop/Alt+/moses/working/train/model/lex.e2f /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/hoang/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hoang/mosesdecoder/scripts/../bin/score /home/hoang/Desktop/Alt+/moses/working/train/model/extract.inv.sorted.gz /home/hoang/Desktop/Alt+/moses/working/train/model/lex.e2f /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Sat Oct  8 17:12:33 2016
/home/hoang/mosesdecoder/scripts/../bin/score /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/extract.0.gz /home/hoang/Desktop/Alt+/moses/working/train/model/lex.e2f /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/run.0.sh/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/run.1.sh/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/run.2.sh/home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/run.3.shgunzip -c /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011  | gzip -c > /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/hoang/Desktop/Alt+/moses/working/train/model/tmp.16011 
Finished Sat Oct  8 17:12:47 2016
(6.6) consolidating the two halves @ Sat Oct  8 17:12:47 ICT 2016
Executing: /home/hoang/mosesdecoder/scripts/../bin/consolidate /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.f2e.gz /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
.......
Executing: rm -f /home/hoang/Desktop/Alt+/moses/working/train/model/phrase-table.half.*
(7) learn reordering model @ Sat Oct  8 17:12:52 ICT 2016
(7.1) [no factors] learn reordering model @ Sat Oct  8 17:12:52 ICT 2016
(7.2) building tables @ Sat Oct  8 17:12:52 ICT 2016
Executing: /home/hoang/mosesdecoder/scripts/../bin/lexical-reordering-score /home/hoang/Desktop/Alt+/moses/working/train/model/extract.o.sorted.gz 0.5 /home/hoang/Desktop/Alt+/moses/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sat Oct  8 17:12:55 ICT 2016
  no generation model requested, skipping step
(9) create moses.ini @ Sat Oct  8 17:12:55 ICT 2016
